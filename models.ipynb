{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MultiHeadAttention, TimeDistributed, MaxPool2D, BatchNormalization, Dense, Input, Reshape, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'saved_np_data/'\n",
    "features_path = os.path.join(data_dir, 'features.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1113, 10, 100, 100, 3)\n",
      "(1113,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples = 1113\n",
    "## Sequence Size = 10\n",
    "## img_dim = (100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (779, 10, 100, 100, 3)\n",
      "y_train shape: (779,)\n",
      "X_test shape: (334, 10, 100, 100, 3)\n",
      "y_test shape: (334,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 22:28:36.185692: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-16 22:28:36.186962: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Mul: Dst tensor is not initialized. [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pw/3vr89zzj1xvbzvtk1yfz9kkc0000gn/T/ipykernel_5330/591213570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2101\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Mul: Dst tensor is not initialized. [Op:Mul]"
     ]
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(32, 3, activation = 'relu'), input_shape = (10,100,100,3)))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(64, 3, activation = 'relu')))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(128, 3, activation = 'relu')))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "# model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# num_heads = 8  # You can adjust the number of attention heads as needed\n",
    "# key_dim = 64  # Adjust the number of units for the attention mechanismx\n",
    "\n",
    "# attention_layer = MultiHeadAttention(key_dim=key_dim, num_heads=num_heads)\n",
    "\n",
    "# # Reshape the output from the previous layers to be 3D (batch_size * num_tokens, height * width, channels)\n",
    "# reshape_layer = TimeDistributed(Flatten())\n",
    "# attention_input = reshape_layer(model.layers[-1].output)\n",
    "\n",
    "# # Apply Multi-Head Attention\n",
    "# attention_output = attention_layer(attention_input, attention_input)\n",
    "# # model.add(Dense(6))\n",
    "\n",
    "\n",
    "input_shape = (10, 100, 100, 3)\n",
    "video_input = Input(shape=input_shape)\n",
    "\n",
    "# Feature Extraction using Conv2D layers with BatchNormalization\n",
    "x = TimeDistributed(Conv2D(32, 3, activation='relu'))(video_input)\n",
    "x = Dropout(0.3)(x)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "\n",
    "x = TimeDistributed(Conv2D(64, 3, activation='relu'))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "\n",
    "x = TimeDistributed(Conv2D(128, 3, activation='relu'))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "\n",
    "# Reshape the output from the previous layers to be 3D (batch_size * num_tokens, height * width, channels)\n",
    "x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "# Multi-Head Attention for Temporal Feature Extraction\n",
    "num_heads = 8  # You can adjust the number of attention heads as needed\n",
    "key_dim = 64  # Adjust the number of units for the attention mechanism\n",
    "\n",
    "# Create the MultiHeadAttention layer\n",
    "attention_layer = MultiHeadAttention(key_dim=key_dim, num_heads=num_heads)\n",
    "\n",
    "# Reshape the output from the previous layers to be 3D (batch_size * num_tokens, height * width, channels)\n",
    "attention_input = Reshape((-1, x.shape[-1]))(x)\n",
    "\n",
    "# Apply Multi-Head Attention\n",
    "attention_output = attention_layer(attention_input, attention_input)\n",
    "\n",
    "x = Flatten()(attention_output)\n",
    "\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(7, activation = 'softmax')(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=video_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10, 100, 10  0           []                               \n",
      "                                0, 3)]                                                            \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDistri  (None, 10, 98, 98,   896        ['input_2[0][0]']                \n",
      " buted)                         32)                                                               \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 10, 98, 98,   0           ['time_distributed_7[0][0]']     \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDistri  (None, 10, 98, 98,   128        ['dropout_4[0][0]']              \n",
      " buted)                         32)                                                               \n",
      "                                                                                                  \n",
      " time_distributed_9 (TimeDistri  (None, 10, 96, 96,   18496      ['time_distributed_8[0][0]']     \n",
      " buted)                         64)                                                               \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 10, 96, 96,   0           ['time_distributed_9[0][0]']     \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " time_distributed_10 (TimeDistr  (None, 10, 96, 96,   256        ['dropout_5[0][0]']              \n",
      " ibuted)                        64)                                                               \n",
      "                                                                                                  \n",
      " time_distributed_11 (TimeDistr  (None, 10, 94, 94,   73856      ['time_distributed_10[0][0]']    \n",
      " ibuted)                        128)                                                              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 10, 94, 94,   0           ['time_distributed_11[0][0]']    \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " time_distributed_12 (TimeDistr  (None, 10, 94, 94,   512        ['dropout_6[0][0]']              \n",
      " ibuted)                        128)                                                              \n",
      "                                                                                                  \n",
      " time_distributed_13 (TimeDistr  (None, 10, 1131008)  0          ['time_distributed_12[0][0]']    \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 10, 1131008)  0           ['time_distributed_13[0][0]']    \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 10, 1131008)  2317436928  ['reshape_2[0][0]',             \n",
      " eadAttention)                                                    'reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10, 512)      579076608   ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 10, 512)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10, 7)        3591        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,896,611,271\n",
      "Trainable params: 2,896,610,823\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.build(input_shape = (10,100,100,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
